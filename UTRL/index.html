<!DOCTYPE html>
<html>
  <head>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&display=swap"
      rel="stylesheet"
    />
    <title>
      Learning to Generate Unit Test via Adversarial Reinforcement Learning
    </title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/icon?family=Material+Icons"
    />
    <link rel="stylesheet" href="assets/css/main.css" />
    <meta
      name="description"
      content="We propose a novel adversarial reinforcement learning framework to generate high-quality unit tests for code language models."
    />
    <meta
      name="author"
      content="Dongjun Lee, Changho Hwang, Kimin Lee"
    />
    <meta
      property="og:title"
      content="Learning to Generate Unit Test via Adversarial Reinforcement Learning"
    />
    <meta
      property="og:description"
      content="We propose a novel adversarial reinforcement learning framework to generate high-quality unit tests for code language models."
    />
    <meta
      name="twitter:title"
      content="Learning to Generate Unit Test via Adversarial Reinforcement Learning"
    />
    <meta
      name="twitter:description"
      content="We propose a novel adversarial reinforcement learning framework to generate high-quality unit tests for code language models."
    />
  </head>

  <body>
    <div class="container">
      <div class="paper-title">
        <h1>
          Learning to Generate Unit Test via Adversarial Reinforcement Learning
        </h1>
      </div>

      <div class="venue">
        ICLR 2026
      </div>

      <div id="authors">
        <center>
          <div class="author-row-new">
            <a href="#">Dongjun Lee<sup>1</sup></a>,
            <a href="#">Changho Hwang<sup>2</sup></a>,
            <a href="#">Kimin Lee<sup>1,†</sup></a>
          </div>
        </center>
        <center>
          <div class="affiliations"><span><sup>1</sup>KAIST</span>&nbsp;&nbsp;&nbsp;<span><sup>2</sup>Microsoft Research</span>&nbsp;&nbsp;&nbsp;</div>
        </center>
        <center>
          <div class="paper-btn-parent">
            <a
              href="https://arxiv.org/abs/2508.21107"
              class="paper-btn arxiv-btn"
              target="_blank"
            >
              <i class="fas fa-file-alt"></i> arXiv
            </a>
            <a
              href="https://github.com/dgjun32/UTRL"
              class="paper-btn code-btn"
              target="_blank"
            >
              <i class="fas fa-code"></i> Code
            </a>
            <a
              href="https://huggingface.co/datasets/dgjun32/UTRL_TRAINING"
              class="paper-btn dataset-btn"
              target="_blank"
            >
              <i class="fas fa-database"></i> Training Data
            </a>
            <a
              href="https://huggingface.co/datasets/dgjun32/UTRL_TACO_EVAL"
              class="paper-btn model-btn"
              target="_blank"
            >
              <i class="fas fa-database"></i> Evaluation Data
            </a>
          </div>
        </center>
      </div>

      <section>
        <h2 style="padding-top: 0px">Overview</h2>
        <hr />

        <p>
          Unit testing is a core practice in programming, enabling systematic evaluation of programs produced by human developers or large language models (LLMs).
          Given the challenges in writing comprehensive unit tests, LLMs have been employed to automate unit test generation, yet methods for training LLMs to produce high-quality unit tests remain underexplored.
        </p>
        <p>
          High-quality tests must do more than execute successfully: they should detect subtle faults
          and reliably separate better code solutions from near-correct ones.
          Therefore, unit test generation is an <strong style="font-weight: 500">open-ended task</strong> with no single fixed answer.
          Existing training pipelines are often bottlenecked by
          <strong style="font-weight: 500">high-quality unit test annotations</strong>, which are expensive and not available at scale.
          This makes pure imitation-based training (e.g., SFT on instruction-test pairs) fundamentally limited.
        </p>
        <p>
          We argue RL is better suited to train LLMs for unit test generation because it can optimize for utility signals of generated tests,
          rather than strictly imitating reference tests. Based on this idea, we propose
          <strong style="font-weight: 500">UTRL</strong>, an adversarial reinforcement learning framework where a
          unit test generator and a code generator provide feedback to each other to improve test quality without relying on heavy reference unit test annotation.
        </p>
      </section>

      <section>
        <h2>Method</h2>
        <hr />

        <p>
          Our key insight is to define rewards for unit test generation based on relationships between unit test generation and code generation.
          In perspective of a code generation, the generated code should pass entire test cases in the unit test. Otherwise, in perspective of a unit test generation,
          the generated unit test should be able to detect subtle errors in arbitrary code solutions, and at the same time, each test case in the unit test should be functionally correct and executable.
          Inspired by this relationship, we train unit test generator and code generator in adversarial manner, where (1) the unit test generator is trained to maximize a discrimination
          reward, encouraging it to produce tests that reveal faults in the code generator’s solutions; and (2) the code generator is trained to maximize a code reward, en-
          couraging it to produce solutions that pass the unit tests generated by the unit test generator.
          In the following section, we provide details of the rewards for training unit test generator, where we utilize the weighted sum of two reward temrs: 
          <b>discrimination reward</b> and <b>validity reward</b>.
        </p>

        <h3>Discrimination reward</h3>
        <p>
          This reward measures whether generated unit tests can distinguish generated code solutions
          (sampled from a code generator LLM) from ground-truth solutions. In short, it quantifies
          how well tests expose subtle errors across multiple candidate programs.
        </p>
        <figure class="figure" style="text-align: center">
          <img
            src="assets/imgs/discrimination_reward.png"
            alt="Discrimination reward"
            style="max-width: 85%; height: auto; margin-top: 0.75em; margin-bottom: 0em"
          />
          <figcaption class="caption" style="text-align: center">
            Discrimination reward illustration.
          </figcaption>
        </figure>

        <h3>Validity reward</h3>
        <p>
          This reward measures how many generated test cases are functionally correct.
          It is computed by executing each test cases in the generated unit tests against the ground-truth code solution, and checking whether the test case can be executed successfully without runtime errors and failed assertions.
          This reward encourages the unit test generator to produce functionally correct and executable test cases, which are essential for reliable code evaluation.
        </p>

        <h3>UTRL</h3>
        <p>
          Based on these rewards, we alternate between training the unit test generator and the code generator. Unit test generator is optimized to maximize the weighted sum of 
          discrimination and validity rewards, while code generator is optimized to generated code solutions that can pass the unit tests generated by the unit test generator.
        </p>
        <figure class="figure" style="text-align: center">
          <img
            src="assets/imgs/algorithm_table.png"
            alt="UTRL training algorithm table"
            style="max-width: 85%; height: auto; margin-top: 0.75em; margin-bottom: 0em"
          />
          <figcaption class="caption" style="text-align: center">
            UTRL training algorithm table
          </figcaption>
        </figure>
        <p>
          By repeating these steps, the code generator produces more higher-quality (near perfect) code solutions, which in turn provide more challenging discrimination task for the unit test generator,
          rendering the unit test generator learn to generate more higher-quality unit tests.
        </p>
      </section>

      <section>
        <h2>Evaluation Metrics</h2>
        <hr />

        <p>
          Ultimately, the quality of a unit test should be judged by how well it captures subtly faulty code.
          Well-known proxy metrics such as Line Coverage and Mutation Score are useful, but they mainly reflect whether tests touch specific execution paths.
          They are limited in evaluating whether a test can catch highly subtle failures, such as code solutions that still pass more than 90% of oracle test cases.
          To enable a more thorough evaluation of generated unit tests, we propose two metrics for LLM-generated unit tests.
        </p>

        <h3>1. Best-of-N Improvement</h3>
        <p>
          We sample candidate code solutions with diverse code LLMs (Qwen3-4B, 8B, 14B, GPT-4o)
          using 32-shot generation. For each task, we use generated unit tests to select a single best candidate,
          then evaluate that selected code with human-curated oracle unit tests.
          This metric captures how well generated tests identify high-quality solutions in practice.
        </p>

        <h3>2. Unit Test Fidelity</h3>
        <p>
          Unit Test Fidelity measures alignment between scores induced by generated unit tests
          and scores induced by human-curated unit tests.
          For each problem, we collect score vectors over hundreds of sampled code solutions,
          then compute Spearman's \(R\) between the oracle-score vector and the generated-test score vector.
          Higher correlation indicates that generated tests induce code evaluation behavior closer to oracle tests.
        </p>
      </section>

      <section>
        <h2>Key Results</h2>
        <hr />
        <figure class="figure" style="text-align: center">
          <img
            src="assets/imgs/best_of_n_improvement.png"
            alt="Best-of-N improvement"
            style="max-width: 85%; height: auto; margin-top: 0.75em; margin-bottom: 0em"
          />
          <figcaption class="caption" style="text-align: center">
            Best-of-N improvement.
          </figcaption>
        </figure>

        <figure class="figure" style="text-align: center">
          <img
            src="assets/imgs/code_generation.png"
            alt="Code generation improvement with UTRL-generated unit tests"
            style="max-width: 85%; height: auto; margin-top: 0.75em; margin-bottom: 0em"
          />
          <figcaption class="caption" style="text-align: center">
            Code generation improvement using UTRL-generated unit tests.
          </figcaption>
        </figure>
        <ul>
          <li>
            <strong>UTRL surpasses supervised fine-tuning:</strong>
            Models trained with UTRL (without ground-truth unit tests) generate higher-quality unit tests
            than models trained with SFT using ground-truth unit tests, showing that learning to detect
            LLM-generated code solutions is more effective than directly imitating reference tests.
          </li>
          <li>
            <strong>Small models trained via UTRL outperform large closed-source models:</strong>
            Qwen3-4B trained with UTRL generates higher-quality unit tests than GPT-4o and GPT-4.1,
            achieving <strong>14.9% accuracy</strong> versus GPT-4o's 10.6% and GPT-4.1's 13.7%
            (when evaluating Qwen3-8B code generation).
          </li>
          <li>
            <strong>Unit tests from small UTRL models improve larger code generators:</strong>
            Unit tests generated by UTRL-trained 4B models effectively improve code generation performance
            of much larger models (e.g., 32B and GPT-4o).
          </li>
        </ul>

        <p>
          Please check our <a href="https://arxiv.org/abs/2508.21107" target="_blank">paper</a> for more details.
        </p>
      </section>

      <section>
        <h2>BibTeX</h2>
        <hr />
        <div id="bibtex">
          <pre>
@inproceedings{lee2026utrl,
  title={Learning to Generate Unit Test via Adversarial Reinforcement Learning},
  author={Lee, Dongjun and Hwang, Changho and Lee, Kimin},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2026},
  url={https://arxiv.org/abs/2508.21107}
}
          </pre>
        </div>
      </section>
    </div>
  </body>
</html>
