<!DOCTYPE html>
<html>
  <head>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <link
      rel="stylesheet"
      href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0-beta3/css/all.min.css"
    />
    <link
      href="https://fonts.googleapis.com/css2?family=IBM+Plex+Sans:ital,wght@0,300;0,400;0,500;0,600;0,700;1,300;1,400;1,500;1,600;1,700&display=swap"
      rel="stylesheet"
    />
    <title>
      Learning to Generate Unit Test via Adversarial Reinforcement Learning
    </title>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="stylesheet"
      href="https://fonts.googleapis.com/icon?family=Material+Icons"
    />
    <link rel="stylesheet" href="assets/css/main.css" />
    <meta
      name="description"
      content="We propose a novel adversarial reinforcement learning framework to generate high-quality unit tests for code language models."
    />
    <meta
      name="author"
      content="Your Name and Collaborators"
    />
    <meta
      property="og:title"
      content="Learning to Generate Unit Test via Adversarial Reinforcement Learning"
    />
    <meta
      property="og:description"
      content="We propose a novel adversarial reinforcement learning framework to generate high-quality unit tests for code language models."
    />
    <meta
      name="twitter:title"
      content="Learning to Generate Unit Test via Adversarial Reinforcement Learning"
    />
    <meta
      name="twitter:description"
      content="We propose a novel adversarial reinforcement learning framework to generate high-quality unit tests for code language models."
    />
  </head>

  <body>
    <div class="container">
      <div class="paper-title">
        <h1>
          Learning to Generate Unit Test via Adversarial Reinforcement Learning
        </h1>
      </div>

      <div class="venue">
        ICLR 2026
      </div>

      <div id="authors">
        <center>
          <div class="author-row-new">
            <a href="#">Dongjun Lee<sup>1</sup></a>,
            <a href="#">Changho Hwang<sup>2</sup></a>,
            <a href="#">Kimin Lee<sup>1,†</sup></a>
          </div>
        </center>
        <center>
          <div class="affiliations"><span><sup>1</sup>KAIST</span>&nbsp;&nbsp;&nbsp;<span><sup>2</sup>Microsoft Research</span>&nbsp;&nbsp;&nbsp;</div>
        </center>
        <center>
          <div class="paper-btn-parent">
            <a
              href="https://arxiv.org/abs/XXXX.XXXXX"
              class="paper-btn arxiv-btn"
              target="_blank"
            >
              <i class="fas fa-file-alt"></i> arXiv
            </a>
            <a
              href="https://github.com/yourusername/utrl"
              class="paper-btn code-btn"
              target="_blank"
            >
              <i class="fas fa-code"></i> Code
            </a>
            <a
              href="https://huggingface.co/datasets/yourusername/utrl-training"
              class="paper-btn dataset-btn"
              target="_blank"
            >
              <i class="fas fa-database"></i> Training Data
            </a>
            <a
              href="https://huggingface.co/datasets/yourusername/utrl-evaluation"
              class="paper-btn model-btn"
              target="_blank"
            >
              <i class="fas fa-database"></i> Evaluation Data
            </a>
          </div>
        </center>
      </div>

      <section>
        <h2 style="padding-top: 0px">Overview</h2>
        <hr />

        <p>
          Unit testing is a core practice in programming, enabling systematic evaluation
          of programs produced by human developers or large language models (LLMs).
          While unit tests serve as crucial verifiable reward functions in RL-based code
          generation, implementing high-quality unit tests remains
          <strong style="font-weight: 500">labor-intensive and challenging</strong>,
          requiring tests to contain functionally valid test cases that cover challenging
          edge cases capable of discriminating subtly faulty code implementations.
        </p>
        <p>
          Existing approaches for training LLMs to generate unit tests typically rely on
          <strong style="font-weight: 500">supervised fine-tuning (SFT)</strong> with
          instruction–unit test pairs. However, these methods face fundamental scalability
          challenges:
        </p>
        <blockquote>
          SFT-based methods require
          <strong style="font-weight: 500">ground-truth unit test annotations</strong>
          for every training example, which are costly to obtain and difficult to scale
          across diverse programming domains. In contrast, RL offers a more scalable
          paradigm by directly optimizing models against reward signals
          <strong style="font-weight: 500">without relying on explicit unit test labels</strong>.
        </blockquote>
        <p>
          We present
          <strong style="font-weight: 500">UTRL</strong>, an adversarial reinforcement
          learning framework that iteratively trains two LLMs in an adversarial manner:
          a <strong style="font-weight: 500">unit test generator</strong> and a
          <strong style="font-weight: 500">code generator</strong>. Our key innovation
          lies in the <em>discrimination reward</em> design, which rewards the unit test
          generator when its generated tests successfully distinguish code solutions
          produced by the code generator from ground-truth solutions—thereby eliminating
          the need for ground-truth unit test annotations.
        </p>

        <figure class="figure" style="text-align: center">
          <img
            src="assets/imgs/overview.png"
            alt="Overview of UTRL framework"
            style="
              max-width: 85%;
              height: auto;
              margin-top: 0.75em;
              margin-bottom: 0em;
            "
          />
          <figcaption class="caption" style="text-align: center">
            The unit test generator is trained to generate unit tests that detect faults in
            code generated by the code generator, while the code generator is trained to
            produce code that passes the generated unit tests.
          </figcaption>
        </figure>

        <p>
          Through this adversarial training, the code generator progressively learns to
          produce solutions harder to distinguish from ground-truth code, while the unit
          test generator learns to generate highly discriminative test cases that detect
          subtle faults in near-correct solutions. On the TACO benchmark, unit tests
          generated by Qwen3-4B trained with UTRL induce
          <strong style="font-weight: 500">3.1× higher code accuracy gain</strong>
          when used for best-of-N sampling, outperform the same model trained via SFT,
          and even surpass frontier models like
          <strong style="font-weight: 500">GPT-4.1 and GPT-4o</strong>
          in generating high-quality unit tests.
        </p>
      </section>

      <section>
        <h2>Problem: Challenges in Unit Test Generation</h2>
        <hr />
        <p>
          Generating unit tests is fundamentally different from other code generation tasks.
          Tests must not only be syntactically correct but also test the right behaviors,
          cover edge cases, and catch potential bugs.
        </p>

        <h3>Observation: Supervised approaches fall short</h3>
        <p>
          We systematically analyze existing supervised learning approaches for unit test
          generation. Our analysis reveals that models trained with standard maximum
          likelihood estimation (MLE) often produce tests that:
        </p>
        <ul>
          <li>Focus on happy paths while missing edge cases</li>
          <li>Generate syntactically correct but semantically weak assertions</li>
          <li>Fail to achieve comprehensive code coverage</li>
          <li>Miss subtle bugs that would be caught by human-written tests</li>
        </ul>

        <figure class="figure" style="text-align: center">
          <img
            src="assets/imgs/problem.png"
            alt="Comparison of supervised vs UTRL"
            style="max-width: 80%"
          />
          <figcaption class="caption" style="text-align: center">
            Supervised models generate tests with lower coverage and weaker assertions
            compared to our UTRL approach.
          </figcaption>
        </figure>

        <h3>
          Hypothesis: Adversarial training provides richer learning signals
        </h3>
        <p>
          We hypothesize that the limitations of supervised approaches stem from the
          <strong style="font-weight: 500">narrow optimization objective</strong>
          of token-level likelihood. This objective fails to capture the
          <strong style="font-weight: 500">holistic quality</strong> of unit tests.
        </p>
        <p>
          An adversarial approach, where a discriminator learns to distinguish between
          human-written and model-generated tests, can provide a more comprehensive
          quality signal. This forces the generator to capture subtle patterns that
          characterize high-quality tests.
        </p>

        <h3>Challenge: Stable training in discrete action spaces</h3>
        <p>
          However, applying adversarial training to discrete sequences like code is
          non-trivial. Standard GAN approaches are unstable for text generation due to
          the discrete nature of tokens. To address this, our UTRL framework incorporates:
        </p>
        <ul>
          <li>Policy gradient methods with carefully designed reward shaping</li>
          <li>A curriculum learning strategy that progressively increases difficulty</li>
          <li>Multi-objective optimization balancing adversarial and auxiliary rewards</li>
        </ul>
      </section>

      <section>
        <h2>Method: Adversarial Reinforcement Learning for Unit Tests</h2>
        <hr />

        <p>
          Our UTRL framework consists of three main components working together:
        </p>

        <blockquote>
          <strong style="font-weight: 500">Generator:</strong> A code LM that generates
          unit tests given source code as input<br/>
          <strong style="font-weight: 500">Discriminator:</strong> A model that learns to
          distinguish between human-written and generated tests<br/>
          <strong style="font-weight: 500">Reward Function:</strong> Combines adversarial
          reward with auxiliary signals (coverage, executability)
        </blockquote>

        <figure class="figure" style="text-align: center">
          <img
            src="assets/imgs/method.png"
            alt="UTRL Architecture"
            style="
              max-width: 85%;
              height: auto;
              margin-top: 0.75em;
              margin-bottom: 0em;
            "
          />
          <figcaption class="caption" style="text-align: center">
            The UTRL framework uses adversarial training between a generator and discriminator
            to produce high-quality unit tests.
          </figcaption>
        </figure>

        <p style="margin-bottom: 8px">
          The training process follows these key steps:
        </p>
        <ol style="margin-top: 0; margin-left: -10px">
          <li>
            The <strong style="color: #1e90ff">generator</strong> produces unit tests
            for given source code
          </li>
          <li>
            The <strong style="color: #1e90ff">discriminator</strong> evaluates whether
            tests appear human-written
          </li>
          <li>
            <strong style="color: #1e90ff">Rewards</strong> are computed based on
            discriminator feedback and auxiliary metrics
          </li>
          <li>
            The generator is updated via policy gradients to maximize expected reward
          </li>
          <li>
            The discriminator is updated to better distinguish real from generated tests
          </li>
        </ol>

        <h3>Reward Shaping</h3>
        <p>
          A key innovation is our carefully designed reward function that balances
          multiple objectives:
        </p>
        <div class="code-block">
R = λ₁ · R_adv + λ₂ · R_coverage + λ₃ · R_exec + λ₄ · R_diversity

where:
  R_adv:       Adversarial reward from discriminator
  R_coverage:  Code coverage achieved by the test
  R_exec:      Test executability (syntax correctness)
  R_diversity: Diversity among generated tests
        </div>
      </section>

      <section>
        <h2>Results</h2>
        <hr />

        <p>
          We evaluate UTRL on multiple benchmarks including HumanEval-Test, APPS-Test,
          and a new benchmark we curated. Results show consistent improvements across
          all metrics:
        </p>

        <figure class="figure" style="text-align: center">
          <img
            src="assets/imgs/results.png"
            alt="Main results table"
            style="
              max-width: 85%;
              height: auto;
              margin-top: 0.75em;
              margin-bottom: 0em;
            "
          />
          <figcaption class="caption" style="text-align: center">
            UTRL achieves significant improvements in code coverage, bug detection rate,
            and test quality across all benchmarks.
          </figcaption>
        </figure>

        <h3>Example: Generated Unit Tests</h3>
        <p>
          Below is a comparison of tests generated by a supervised baseline versus UTRL
          for a binary search implementation:
        </p>

        <div class="code-block">
# Source Code
def binary_search(arr, target):
    left, right = 0, len(arr) - 1
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1

# Supervised Baseline
def test_binary_search():
    assert binary_search([1, 2, 3, 4, 5], 3) == 2

# UTRL (Ours)
def test_binary_search():
    # Normal case
    assert binary_search([1, 2, 3, 4, 5], 3) == 2
    # Edge case: target not found
    assert binary_search([1, 2, 3, 4, 5], 6) == -1
    # Edge case: empty array
    assert binary_search([], 1) == -1
    # Edge case: single element
    assert binary_search([5], 5) == 0
    # Edge case: target at boundaries
    assert binary_search([1, 2, 3, 4, 5], 1) == 0
    assert binary_search([1, 2, 3, 4, 5], 5) == 4
        </div>

        <p>
          Our model generates more comprehensive tests that cover edge cases such as
          empty arrays, single elements, and boundary conditions.
        </p>

        <h3>Quantitative Results</h3>
        <p>
          UTRL achieves state-of-the-art performance across all evaluation metrics:
        </p>
        <ul>
          <li><strong>Code Coverage:</strong> 85.3% (↑40% over baseline)</li>
          <li><strong>Bug Detection Rate:</strong> 78.1% (↑35% over baseline)</li>
          <li><strong>Test Executability:</strong> 94.7% (↑12% over baseline)</li>
          <li><strong>Human Evaluation Score:</strong> 4.2/5.0 (↑28% over baseline)</li>
        </ul>
      </section>

      <section>
        <h2>BibTeX</h2>
        <hr />
        <div id="bibtex">
          <pre>
@inproceedings{yourname2026utrl,
  title={Learning to Generate Unit Test via Adversarial Reinforcement Learning},
  author={Your Name and Collaborators},
  booktitle={International Conference on Learning Representations (ICLR)},
  year={2026}
}</pre>
        </div>
      </section>
    </div>
  </body>
</html>
